Your lab report should be brief (maximum of 2 pages) and contain the collowing

• Describe any design decisions you made.

• Discuss and justify any changes you made to the API.

• Describe any missing or incomplete elements of your code.

• Describe how long you spent on the lab, and whether there was anything you found particularly difficult or confusing.

1. Exercise 1 - `filter_op.go` and `join_op.go`

- I've implemented the Filter operator by 1) `Descriptor()` simply returning the descriptor of the child operator, 2) `Iterator()` returning a new iterator that iterates over the child operator's iterator and only returns the tuples that satisfy the predicate.

- For the Join operator, I've wrote two implementations - simple nested loop join and block hash join. The nested loop join simply iterates over the left child operator once and for each left tuple, iterates over the right child operator, which is inefficient. To make it much faster while satisfying the condition to not exceed `maxBufferSize`, I've implemented the block hash join, which builds a partial hash table for the right child operator and iterates over the left child operator (and repeat this until we exhaust all right tuples). This way, I was able to speed up the join operation significantly. However, I wasn't able to pass the `TestJoinBigOptional` test due to the slow performance of `insertTuple` function (the join operation itself is fast enough). I plan to spend more time on `insertTuple` function to make it faster.

2. Exercise 2 - `agg_state.go` and `agg_op.go`

- For `agg_state.go`, I've implemented the four aggregation operators, including SUM, AVG, MIN, and MAX. I need to maintain a number representing the running aggregation result, and I chose sum for SUM and AVG, min for MIN, and max for MAX. This way, it is possible to return the aggregation result with a simple computation when `Finalize()` is called.
- For `agg_state.go`, I've simply implemented the given three functions - `Descriptor()`, `extractGroupByKeyTuple`, and `getFinalizedTuplesIterator`. I think the core idea here was to join the "group tuple" with all "aggregation result tuples".

3. Exercise 3 - `insert_op.go` and `delete_op.go`

- The implementations for insert and delete operators are very similar. First, I chose to maintain two fields, `insertFile` (or `deleteFile`) and `child`. `insertFile` (or `deleteFile`) is to access the file when executing the insert (or delete) operation, and `child` is to access the child operator's iterator, just like other operators. When `Iterator()` is called, I simply return a new iterator that iterates over the child operator's iterator, inserts (or deletes) all the tuples, and returns the count of inserted (or deleted) tuples. Here, I added a `done` local variable to check if the operation is done or not, so that the iterator simply returns `nil` when the operation is already done.

4. Exercise 4 - `project_op.go`

- To support the DISTINCT keyword, I added a boolean field `distinct` to the `Project` struct. When `Iterator()` is called, I initialize an empty map to store the tuples that are already returned. Specifically, `Tuple.tupleKey()` is used as the key of the map. If the `distinct` field is true, whenever before returning a tuple, I check if the tuple is already in the map. If it is, I skip the tuple and otherwise, I return the tuple and add it to the map.
- One additional core idea to implement the `Project` operator was to properly replace the field names with `outputNames`. When calling `.project()` to a tuple, I used the original field types, and then replaced the tuple descriptor right before returning the tuple.

5. Exercise 5 - `order_by_op.go`