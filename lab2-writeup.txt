Your lab report should be brief (maximum of 2 pages) and contain the collowing

• Describe any design decisions you made.

• Discuss and justify any changes you made to the API.

• Describe any missing or incomplete elements of your code.

• Describe how long you spent on the lab, and whether there was anything you found particularly difficult or confusing.

1. Exercise 1 - `filter_op.go` and `join_op.go`

- I've implemented the Filter operator by 1) `Descriptor()` simply returning the descriptor of the child operator, 2) `Iterator()` returning a new iterator that iterates over the child operator's iterator and only returns the tuples that satisfy the predicate.
- For the Join operator, I've wrote two implementations - simple nested loop join and block hash join. The nested loop join simply iterates over the left child operator once and for each left tuple, iterates over the right child operator, which is inefficient. To make it much faster while satisfying the condition to not exceed `maxBufferSize`, I've implemented the block hash join, which builds a partial hash table for the right child operator and iterates over the left child operator (and repeat this until we exhaust all right tuples). This way, I was able to speed up the join operation significantly. However, I wasn't able to pass the `TestJoinBigOptional` test due to the slow performance of `insertTuple` function (the join operation itself is fast enough). I plan to spend more time on `insertTuple` function to make it faster.
- Algorithm trade-off: Simple nested loop join uses minimal memory, as it iterates over all inner tuples for each outer tuple. This makes it very slow (I/O complexity is O(N + N*M)). On the other hand, block hash join is much faster as it builds a hash table for the inner tuples (as large as possible within the buffer limit). For each outer tuple, we can simply look up the hash table to find the matching inner tuples. Instead, it uses more memory than the nested loop join. Also, there is additional cost to build the hash table. In summary, the trade-off is between memory usage and speed.

2. Exercise 2 - `agg_state.go` and `agg_op.go`

- For `agg_state.go`, I've implemented the four aggregation operators, including SUM, AVG, MIN, and MAX. I need to maintain a number representing the running aggregation result, and I chose sum for SUM and AVG, min for MIN, and max for MAX. This way, it is possible to return the aggregation result with a simple computation when `Finalize()` is called.
- For `agg_state.go`, I've simply implemented the given three functions - `Descriptor()`, `extractGroupByKeyTuple`, and `getFinalizedTuplesIterator`. I think the core idea here was to join the "group tuple" with all "aggregation result tuples".

3. Exercise 3 - `insert_op.go` and `delete_op.go`

- The implementations for insert and delete operators are very similar. First, I chose to maintain two fields, `insertFile` (or `deleteFile`) and `child`. `insertFile` (or `deleteFile`) is to access the file when executing the insert (or delete) operation, and `child` is to access the child operator's iterator, just like other operators. When `Iterator()` is called, I simply return a new iterator that iterates over the child operator's iterator, inserts (or deletes) all the tuples, and returns the count of inserted (or deleted) tuples. Here, I added a `done` local variable to check if the operation is done or not, so that the iterator simply returns `nil` when the operation is already done.

4. Exercise 4 - `project_op.go`

- To support the DISTINCT keyword, I added a boolean field `distinct` to the `Project` struct. When `Iterator()` is called, I initialize an empty map to store the tuples that are already returned. Specifically, `Tuple.tupleKey()` is used as the key of the map. If the `distinct` field is true, whenever before returning a tuple, I check if the tuple is already in the map. If it is, I skip the tuple and otherwise, I return the tuple and add it to the map.
- One additional core idea to implement the `Project` operator was to call `EvalExpr` for each selected field, and using the descriptor from `Descriptor()` to build the new tuple.

5. Exercise 5 - `order_by_op.go`
- I've added a bool[] field `ascending` to the `OrderBy` struct to store whether to sort in ascending or descending order for each field. The `Descriptor()` method simply returns the descriptor of the child operator.
- `Iterator()`: First, I implemented `multiSorter` which enables sorting a slice of tuples by multiple less functions. This is done by implementing the `sort.Interface` interface. Then, I wrote a helper method `sort` for `OrderBy` struct, which sorts the tuples by creating a `multiSorter` with proper less functions, and then sorting the tuples using the `sort.Sort()`. The iterator sorts the tuples at once beforehand, and then returns the sorted tuples one by one when called.

6. Exercise 6 - `limit_op.go`
- The limit operator's implementation is very simple. `limitTups` field maintains the `Expr` that always returns the limit count when `EvalExpr` is called. When `Iterator()` is called, I simply return an iterator that returns a tuple from the child operator's iterator if the current count is less than the limit count. Otherwise, it returns `nil`.

7. Exercise 7 - Easy parser
- While debugging `easy_parser_test.go`, I fixed several bugs in `project_op.go` (I originally used `project()` method here instead of `EvalExpr()`), and `heap_file.go` (to replace tuple's descriptor with heap file's descriptor when returning a tuple from the iterator).